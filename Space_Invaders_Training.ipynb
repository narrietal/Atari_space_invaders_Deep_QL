{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RODXQ3gvBbsP"
   },
   "source": [
    "# Space Invaders - Double-Q-learning Training\n",
    "\n",
    "Authors: Nicol√°s Arrieta Larraza and Thuany Stuart\n",
    "\n",
    "Date: 14/05/2021\n",
    "\n",
    "---\n",
    "\n",
    "This notebook describes the learning algorithm and its implementation. The result analysis will be done in the *Space Invaders - Result Analysis* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu9uQ8KhX_w0"
   },
   "source": [
    "## The Environment - Space Invaders Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dt3vqf-YHXv"
   },
   "source": [
    "Space Invaders is an Atari game where the player controls a space ship with the goal of defeating an alien invation. The player has to shoot to destroy the aliens and dodge from the aliens shots to stay alive.\n",
    "\n",
    "We have chosen to work with the RAM observation. In this case, the observation of the environment is given by the state of the RAM, i.e., an array with 128 values. Each value is a byte (integer from 0-255). The RAM is a compact representation of the state in comparison to algorithms the display image as the state.\n",
    "\n",
    "In this environment, the player can perform 6 actions: NOOP, RIGHT, LEFT, FIRE, RIGHTFIRE, and LEFTFIRE. Notice that, by default, each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from {2,3,4}. In this case, the agent skips some frames in which it is not able to perform any other action, therefore, k is called the *frameskip*.\n",
    "\n",
    "The reward in the Space Invaders gym environment is the score at the given time step. We have observed that different invasors (aliens) have different scores. For example, aliens from the first line have score 5 while aliens from the second line have score 10. When no alien is destroied, the reward is zero. There is no negative reward for losing lives, but the information of the number of lives can be read from gym the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZreBs-eYc2x"
   },
   "source": [
    "## The Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUJ2e4MhQjDO"
   },
   "source": [
    "Because Space Invaders is a complex game with 6 actions and a big number of states, we have chosen to use a valua approximation algorithm: Double-Q-Learning. Here, we give a brief explanatoin of the Double-Q-Learning algorithm and its precedents: Q-Learning and Deep-Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-dL9av8YsKm"
   },
   "source": [
    "### Q-Learning\n",
    "\n",
    "Q-learning is an off-policy temporal difference algorithm. Q-learning estimates the action-value at a time $t$ for an action $a_t$ in a given state $s_t$ based on the difference between the expected action-value $Q(s_t, a_t)$, and the current action-value, $r_{t+1} + \\gamma Q(s_{t+1}, a_t)$, where $\\gamma$ is a discount factor for future rewards and $r_{t+1}$ is the reward at the time $t$. The q-value update can be written as:\n",
    "\n",
    "$Q(s_t, a_t) \\leftarrow (1 - \\alpha)Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q(s_{t+1}, a_t)]$\n",
    "\n",
    "With the introduction of the learning rate $\\alpha$. Using the next action as the optimal action, i.e., the one that maximizes the value, we have:\n",
    "\n",
    "$Q(s_t, a_t) \\leftarrow (1 - \\alpha)Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma  \\underset{a}{\\max}(Q(s_{t+1}, a_t))]$\n",
    "\n",
    "Or\n",
    "\n",
    "$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma  \\underset{a}{\\max}(Q(s_{t+1}, a_t)) - Q(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhxWzcP2YuPu"
   },
   "source": [
    "### Deep-Q-Learning\n",
    "\n",
    "The Q-learning algorithm is not suitable for high-dimensional state or action space, because the current action-value estimations have to be stores for each state-action pair. For that purpose, Deep-Q-learning was created. Deep-Q-Learning is a value approximation method, in which a Deep Neural Network (DNN) is used to estimate the Q-values for a given state. For a problem with a $n$ dimensional state and an $m$ actions, the DNN will learn a mapping from $\\mathbb{R}^n$ to $\\mathbb{R}^m$. \n",
    "\n",
    "The objective is to now learn the weights $\\theta_i$ Q-network, and can be written as:\n",
    "\n",
    "$\\theta_{i+1} \\leftarrow \\theta_i + \\alpha [r + \\gamma \\underset{a'}{\\max} Q(s', a', \\theta_{i}) - Q(s, a, \\theta_i)]\\nabla_{\\theta_i} Q(s, a, \\theta_i) $\n",
    "\n",
    "Where $s', a'$ are the next state and action, respectivelly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lz1CZAF4Y0Lz"
   },
   "source": [
    "### Double-Q-Learning\n",
    "\n",
    "In recent works, it was shown that using the same network to choose the best action and to evaluate the q-values leads to overestimation of the q-values. The Double Q learning algorithm gets around this by keeping\n",
    "two DPP models. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. Here, we will refer to the DNN used to determine the greedy policy as (main) model/network and the other one as target model/network. At every number of steps, the weights of the main model are coppied to the target model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB2NOV8zV1w6"
   },
   "source": [
    "## The Learning Process\n",
    "\n",
    "Besides the choice of algorithm parameters, a few implementation decisions had to be made. Here we describe our decisions regarding some aspects of the code. For the implementation of this algorithm, we have taken inspiration from three tutorials:\n",
    "\n",
    "- [Training Deep Q Learning and Deep Q Networks (DQN) Intro and Agent from sentdex](https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/)\n",
    "- [Deep Q-Learning Tutorial: minDQN from Maik Wang](https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc)\n",
    "- [Deep Reinforcement Learning to play Space Invaders from Nihit Desai and Abhimanyu Banerjee](https://nihit.github.io/resources/spaceinvaders.pdf)\n",
    "\n",
    "### Epsilon-greedy exploration policy\n",
    "We have used the $\\epsilon$-greedy exploration policy to balance between exploration and exploitation.\n",
    "\n",
    "### Epsilon decay function\n",
    "At first we implemented a version in which the epsilon decayed exponentially with respect to a decay factor, however we observed that a linear function showed a better performance.\n",
    "\n",
    "### Reward variation\n",
    "\n",
    "We have tried to bound the rewards with values of the set {-1,0,1} where:\n",
    "* -1 was given in case of life loss\n",
    "*  1 was given in case of alien kill\n",
    "*  0 otherwise\n",
    "\n",
    "This approach was supposed to speed up the neural network training. Nevertheless, it resulted on a bad performance since the player did not move or shoot. \n",
    "\n",
    "We reached the conclusion that we should not bound the rewards since not all aliens give the same reward and that the model should learn that killing the aliens of the last row it is more worth in the long term.\n",
    "\n",
    "### Terminal state\n",
    "For a not terminal state, the estimated Q-value is the current reward summed with the discounted future reward. But for a terminal state, the expected Q-value is just the current reward, cause no future reward can be obtained. The step in which the game ends is a terminal reward. We have also included the step in which a life is lost as a terminal reward. We notice that this change helps the agent learn to avoid losing lives.\n",
    "\n",
    "### Normalizing input\n",
    "The input of the neural network consists of arrays of 128 bytes (RAM values) each of them expressed in a number between 0 and 255. We observed that normalizing the input by dividing it by 255 showed a better performance.\n",
    "\n",
    "### Target Model weights' update frequency\n",
    "We tried different configurations for the frequency in which the target model weights are updated including 100, 1000 and 10000 steps. The latter one has yield the best results.\n",
    "\n",
    "### Network Architecture\n",
    "We have implemented the same network as described in the [Deep Reinforcement Learning to play Space Invaders](https://nihit.github.io/resources/spaceinvaders.pdf) paper. It is a fully connnected network with 2 hidden layers. More details and explanations can be found in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsKGWhsfYDcR"
   },
   "source": [
    "## Initial Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpCVpnDLBbsV"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgzkqvY_BbsW"
   },
   "outputs": [],
   "source": [
    "!apt update && apt install xvfb\n",
    "!pip install gym-notebook-wrapper\n",
    "!apt update && apt install python-opengl ffmpeg\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import gym\n",
    "import gnwrapper\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut1ded04XsP2"
   },
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFT6xpD9Xn4-"
   },
   "source": [
    "We define the path for the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WDO97wLrRvE"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/0 Study/Data Science Master/Semester 2/AI/Reinforcement Learning/Lab 5/'\n",
    "version_name = \"fv_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX7EkP-hXzEY"
   },
   "source": [
    "## Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGO-9ZfTYYtR"
   },
   "source": [
    "We define an agent to implement the Double-Q-learning Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiJRhjIuFJyk"
   },
   "outputs": [],
   "source": [
    "class DeepQLearningAgent():\n",
    "  def __init__(self, env):\n",
    "    \"\"\"\n",
    "      Initializes the class and sets the environment\n",
    "    \"\"\"\n",
    "    self.env = env\n",
    "\n",
    "  def get_approximator(self, gd_learning_rate):\n",
    "    \"\"\"\n",
    "      Returns an ANN that is used to approximate the value function.\n",
    "      Input: feature vector representing a state\n",
    "      Output: vector representing the value of each action at the input state\n",
    "    \"\"\"\n",
    "\n",
    "    # Gets the number of actions and the shape of each state\n",
    "    state_shape = env.observation_space.shape\n",
    "    number_of_actions = env.action_space.n\n",
    "\n",
    "    # He Uniform initializes the weights with a zero-mean gaussian distribution,\n",
    "    # where the variance is scaled by the number of input neurons in the layer\n",
    "    # It is indicated to be used with relu activation\n",
    "    init = HeUniform()\n",
    "\n",
    "    # We use Relu activation to avoid vanishing gradients\n",
    "    # We use two dense layers to extract the features of the RAM input array\n",
    "    # We use a linear output layer to estimate the Q action-value\n",
    "    model = Sequential([\n",
    "        Dense(512, input_shape=state_shape, activation='relu', kernel_initializer=init),\n",
    "        Dense(128, activation='relu', kernel_initializer=init),\n",
    "        Dense(number_of_actions, activation='linear', kernel_initializer=init)\n",
    "    ])\n",
    "\n",
    "    # We use Huber loss because it is less sensitive to outliers than squared error\n",
    "    # We use Adam optimizer to optimizer and accuracy as the metric\n",
    "    model.compile(loss=Huber(), optimizer=Adam(lr=gd_learning_rate), metrics='accuracy')\n",
    "    return model\n",
    "\n",
    "  def show_approximator(self):\n",
    "    \"\"\"\n",
    "      Shows the summary of the DNN approximator\n",
    "    \"\"\"\n",
    "    if not self.model is None:\n",
    "      self.model.summary()\n",
    "    else:\n",
    "      print(\"No model was set!\")\n",
    "\n",
    "  def preprocess_state(self, state, reshape=False):\n",
    "    \"\"\"\n",
    "      Returns the normalized state (MinMax scaler) in the format required by the DNN\n",
    "    \"\"\"\n",
    "    state = state/255\n",
    "\n",
    "    # Reshape is used to add an array around the state\n",
    "    # when predicting the action-value for a single state\n",
    "    if reshape:\n",
    "      state = state.reshape(1, -1)\n",
    "    return state\n",
    "\n",
    "  def train_approximator(self, replay_memory, gamma, mini_batch_size, min_replay_size, life_lost):\n",
    "    \"\"\"\n",
    "      Trains the main model with a mini_batch of the replay memory and the defined parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Only train the approximator if there's a minimum number of samples on the \n",
    "    # replay memory to guarantee time independence of samples\n",
    "    if len(replay_memory) < min_replay_size:\n",
    "      return\n",
    "\n",
    "    # Randomly sample a number of time steps from the memory\n",
    "    mini_batch = random.sample(replay_memory, mini_batch_size)\n",
    "    # Get the state before the action and predict the action-value Q(s, a)\n",
    "    states_t = np.array([self.preprocess_state(observation) for (observation, action, reward, new_observation, done) in mini_batch])\n",
    "    q_list_t = self.model.predict(states_t)\n",
    "    # Get the state after the action and predict the action-value Q(s', a') for the next state\n",
    "    states_t_plus_one = np.array([self.preprocess_state(new_observation) for (observation, action, reward, new_observation, done) in mini_batch])\n",
    "    q_list_t_plus_one = self.target_model.predict(states_t_plus_one)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Update the action-value of the actions taken\n",
    "    for i, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done and not life_lost:     # Not a terminal state\n",
    "          # New action-value is the step's reward plus the discounted future reward\n",
    "          max_return = reward + gamma * np.max(q_list_t_plus_one[i][0])\n",
    "        else:                              # Terminal state\n",
    "          # New action-value is is the step's reward\n",
    "          max_return = reward\n",
    "\n",
    "        # Update the action-value of the taken action\n",
    "        current_qs = q_list_t[i]\n",
    "        current_qs[action] = max_return\n",
    "\n",
    "        # Add to dataset\n",
    "        X.append(self.preprocess_state(observation)) \n",
    "        y.append(current_qs)\n",
    "\n",
    "    # Trains model on created dataset\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    self.model.fit(X, y, verbose=0, shuffle=True)\n",
    "\n",
    "  def train(self, \n",
    "            n_episodes,           # number of episodes to train\n",
    "            max_epsilon,          # maximum exploration rate\n",
    "            min_epsilon,          # minimum exploration rate\n",
    "            gamma,                # discount rate\n",
    "            gd_learning_rate,     # learning rate for the gradient descent\n",
    "            mini_batch_size,      # number of samples taken from the replay memory for training\n",
    "            min_replay_size):     # minimum replay size to train (does not train before gathering this amount of observations)         \n",
    "\n",
    "    # Initialize model and target model\n",
    "    self.model = self.get_approximator(gd_learning_rate)\n",
    "    self.target_model = self.get_approximator(gd_learning_rate)\n",
    "    # Set the weights of the target model as the main model\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    TARGET_UPDATE_FREQ = 10000\n",
    "    TRAIN_FREQ = 4\n",
    "\n",
    "    # Initialize stats and counters\n",
    "    reward_per_ep = [] # Total score per episode \n",
    "    steps_to_update_weights = 0  # Steps since last target update\n",
    "    action_distribution = ( # Counts how many times each actions were chosen per episode (by exploitation)\n",
    "        np.array([np.zeros(n_episodes), np.zeros(n_episodes), np.zeros(n_episodes),\n",
    "        np.zeros(n_episodes), np.zeros(n_episodes), np.zeros(n_episodes)]))\n",
    "    \n",
    "    # Initialize replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Define initial epsilon and epsilon decay linearly\n",
    "    epsilon = max_epsilon\n",
    "    epsilon_decay = (max_epsilon - min_epsilon)/n_episodes\n",
    "\n",
    "    # Run episodes\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "      # Initialize state and variables\n",
    "      observation = self.env.reset()\n",
    "      done = False\n",
    "      prev_life = 3\n",
    "      total_ep_reward = 0 # Cummulative episode reward\n",
    "      ep_step = 0 # Episode step count\n",
    "      \n",
    "      while not done:\n",
    "        # Epsilon-greedy exploration strategy\n",
    "        if np.random.uniform() <= epsilon:            # Exploration\n",
    "          # Choose a random action\n",
    "          action = self.env.action_space.sample()\n",
    "        else:                                         # Exploitation\n",
    "          # Predict the action-values\n",
    "          state = self.preprocess_state(observation, reshape=True)\n",
    "          q_list = self.model.predict(state).flatten()\n",
    "          # Choose the optimum policy, the one that maximizes the value\n",
    "          action = np.argmax(q_list)\n",
    "          # Update action distribution stats\n",
    "          action_distribution[action][episode] += 1\n",
    "\n",
    "        # Add step to replay memory\n",
    "        new_observation, reward, done, info = self.env.step(action)\n",
    "        replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "        # Check if a life was lost\n",
    "        life = env.ale.lives()\n",
    "        if life - prev_life == 0:\n",
    "          life_lost = False\n",
    "        else:\n",
    "          life_lost = True\n",
    "        prev_life = life\n",
    "\n",
    "        # Update state, counters, and stats\n",
    "        observation = new_observation\n",
    "        ep_step += 1\n",
    "        steps_to_update_weights+=1\n",
    "        total_ep_reward += reward\n",
    "\n",
    "        # Train main model every number of steps\n",
    "        if ep_step % TRAIN_FREQ == 0 or done:\n",
    "          self.train_approximator(replay_memory, gamma, mini_batch_size, min_replay_size, life_lost)\n",
    "      \n",
    "      # Update target model's weights every number of steps\n",
    "      if steps_to_update_weights >= TARGET_UPDATE_FREQ:\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        steps_to_update_weights = 0\n",
    "\n",
    "      # Update epsilon linearly\n",
    "      epsilon = epsilon - epsilon_decay\n",
    "      \n",
    "      # Save total reward per episode\n",
    "      reward_per_ep.append(total_ep_reward)\n",
    "\n",
    "      # Display information for the user\n",
    "      if episode % 300 == 0 or episode == n_episodes-1:\n",
    "        print(\"Episode %d\"%episode)\n",
    "        self.env.display()\n",
    "\n",
    "    # Close the environment\n",
    "    self.env.close()\n",
    "\n",
    "    return reward_per_ep, action_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rie1OvLXs8cH"
   },
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cR2eiDfxsiu"
   },
   "source": [
    "Here, we define the hyperparameters of the training. We have chosen the hyper parameters based on two papers:\n",
    "\n",
    "- [Playing Atari with Deep Reinforcement Learning from DeepMind](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "- [Deep Reinforcement Learning to play Space Invaders from Nihit Desai and Abhimanyu Banerjee](https://nihit.github.io/resources/spaceinvaders.pdf)\n",
    "\n",
    "We have used 600 episodes for training because we have limited time resources and using Google Colaboratory GPU, the training took in average 6 hours. We note that based on the cited papers, an increase in the training time could significantly the performance of the agents.\n",
    "\n",
    "For the analysis, we have varied the value of $\\gamma$, i.e. the discount factor. The results will be showed in the *Space Invaders - Result Analysis* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxbJLhtuKpeR"
   },
   "outputs": [],
   "source": [
    "N_EPISODES = 600\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "GAMMA = 0.7\n",
    "GD_LEARNING_RATE = 0.0002\n",
    "MINI_BATCH_SIZE = 32\n",
    "MIN_REPLAY_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r69dgc0PtAuW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBQschluwcF4"
   },
   "source": [
    "Firstly, we create the Space Invaders environment. We use the gym notebook wrapper to plot and save videos of episodes every number of iterations to visualize the progress of the agent.\n",
    "\n",
    "By default, the *frameskip* is stochastic and makes the learning harder by adding randomness to the environment. In this setting, we have set the set the *frameskip* to 3 to facilitate the training. The *frameskip* of 3 is the highest *frameskip* such that the agent is still able to see the shots, since the shots are not shown at every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBEPMJhiEAkF"
   },
   "outputs": [],
   "source": [
    "# If the folder already exists, we delete it\n",
    "video_path = path + \"videos_\" + version_name\n",
    "if os.path.exists(video_path):\n",
    "  for filename in os.listdir(video_path):\n",
    "    os.remove(video_path + \"/\" + filename)\n",
    "  os.rmdir(video_path)\n",
    "\n",
    "# Create environment with the wrapper to save videos when displaying the environment\n",
    "env = gnwrapper.Monitor(gym.make('SpaceInvaders-ram-v0', frameskip=3), directory=path + \"videos_\" + version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HIF0pFewwlY"
   },
   "source": [
    "We create an agent with the created environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kJ8104FNz_M"
   },
   "outputs": [],
   "source": [
    "agent = DeepQLearningAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KFJGDrXw23P"
   },
   "source": [
    "And finally, we train the agent with the specified hyper parameters. We can see a progress bar with the episodes trained, and we can see videos of some episodes trained at the beginning, middle and end of the training sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui_LBffBN4wr"
   },
   "outputs": [],
   "source": [
    "reward_per_episode, action_distribution = agent.train(\n",
    "    N_EPISODES,\n",
    "    MAX_EPSILON, MIN_EPSILON, GAMMA,\n",
    "    GD_LEARNING_RATE, MINI_BATCH_SIZE,\n",
    "    MIN_REPLAY_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnJt5fXStGIt"
   },
   "source": [
    "### Generating output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiBJbmjJvjse"
   },
   "source": [
    "We plot the number of times each action was chosen by exploitation per episode. These plots were helpful for us to debug the algorithm, because in some configurations tested, the agent would behave in such a way that only a few actions were chosen at the end of training. For example, many agents would just go to the end of the screen and shoot, not trying to hide from the shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o617GYb-fFzw"
   },
   "outputs": [],
   "source": [
    "# Get the name of each action\n",
    "action_names = env.unwrapped.get_action_meanings()\n",
    "\n",
    "# Plot a subplot for each action distribution\n",
    "fig=plt.figure(figsize=(20, 12))\n",
    "for i in range(env.action_space.n):\n",
    "  plt.subplot(2, 3, i+1)\n",
    "  plt.plot(range(N_EPISODES), action_distribution[i], label=action_names[i])\n",
    "  plt.legend()\n",
    "  plt.title('Distribution of action {} per episode'.format(action_names[i])) \n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Save file\n",
    "fig.savefig(path+'actions_per_episode_' + version_name + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubVT_7cqtXC3"
   },
   "source": [
    "We plot the score per episode and save both the plots and the array with the data. This plot will be used to observe if the learning of the agent. If the score per episode is increasing, the agent is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcvipynD5BTm"
   },
   "outputs": [],
   "source": [
    "  # Plot\n",
    "  plt.title('Total reward per episode') \n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Reward')\n",
    "  plt.plot(range(N_EPISODES), reward_per_episode)\n",
    "\n",
    "  # Save files\n",
    "  plt.savefig(path + 'reward_per_episode_model_' + version_name + '.png')\n",
    "  np.savetxt(path + 'reward_per_episode_' + version_name + '.csv', np.array(reward_per_episode), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDakZyWftJrv"
   },
   "source": [
    "Finally, we save the action-value model trained by the agent so we can test it and compare with the other models in the testing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NFe3WpNqgN5"
   },
   "outputs": [],
   "source": [
    "agent.model.save(path + 'model_' + version_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YsKGWhsfYDcR"
   ],
   "name": "Space_Invaders_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
